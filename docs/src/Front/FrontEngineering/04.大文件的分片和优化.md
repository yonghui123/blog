<!-- ---
title: 04.大文件的分片和优化
date: 2024-10-26
cover: 
categories:
 - 前端工程化
tags:
 - 前端造轮子
description: 
--- -->


## 如何减少页面阻塞

分片上传的一个首要目标是要尽量避免相同的分片重复上传，服务器必须要能够识别来自各个客户端的各个上传请求中，是否存在于过去分片相同的上传请求，所以服务端需要识别哪些分片是相同的。

我们定义文件内容一样的文件称为相同的文件，但是如果对比文件内容的二进制是一个非常耗时的操作，研究了之后发现可以选择基于文件内容的hash值来判断文件是否相同。

所以整个发送文件的流程就可以大概总结为：
1. 客户端计算分片的hash值，并将该hash值发送给服务端
2. 服务端通知客户端有无对应分片
3. 如果没有该文件分片，将该分片发送给服务端，服务端存储

对整个文件也需要计算整个文件的hash值，判断该文件是否上传过了，如果上传过，则不再上传，直接返回。

根绝上图的流程，可以知道，客户端有两件很重要的事情：
1. 计算分片的hash值
2. 计算整个文件的hash值

计算hash值是一个CPU密集型的任务，所以需要一个处理测罗防止其阻塞主线程。

因此可以对整个大文件上传做一个大胆的假设：绝大部分的文件上传都是新的文件上传

在这个假设的基础上，在处理文件的时候，就不需要等待整个文件的hash值计算结果，直接上传分片，同时把分片操作使用多4线程+异步的方式进行上传处理。

### upload-core中的通用函数

**EventEmitter**
统一前后端涉及到的基于各种事件的处理，使用发布订阅模式提供统一的EventEmitter类。比如
1. 前端可能出现的事件：上传进度改变事件，上传暂停/上传恢复事件
2. 后端可能出现的事件：分片写入完成事件，分片合并完成事件

```typescript
export class EventEmitter<T extends string> {
  private events: Map<T, Set<Function>>;
  constructor() {
    this.events = new Map();
  }

  on(event: T, listener: Function) {
    if(!this.events.has(event)){
      this.events.set(event, new Set());
    }
    this.events.get(event).add(listener);
  }

  off(event: T, listener: Function) {
    if(!this.events.has(event)) {
      return;
    }
    this.events.get(event).delete(listener);
  }

  once(event: T, listener: Function) {
    const onceListener = (...args: any[]) => {
      listener(...args);
      this.off(event, onceListener);
    }
    this.on(event, onceListener);
  }

  emit(event: T, ...args: any[]) {
    if(!this.events.has(event)) {
      return;
    }
    this.events.get(event).forEach(listener => listener(...args));
  }
}
```

**TaskQueue**
为支撑前后端的多任务并发执行，提供TaskQueue类，并发任务比如：
1. 前端可能得并发执行：并发请求
2. 后端可能得并发执行：并发的分片Hash校验

```typescript
export class Task {
  fn: Function; // 任务关联的执行函数
  payload?: any; // 任务关联的其他信息
  constructor(fn: Function, payload?: any) {
    this.fn = fn;
    this.payload = payload;
  }

  run() {
    return this.fn(this.payload);
  }
}

// 可并发执行的任务队列
export class TaskQueue extends EventEmitter<'start' | 'pause' | 'drain'> {
  // 待执行的任务
  private tasks: Set<Task> = new Set();
  // 当前正在执行的任务数
  private currentCount = 0;
  // 任务状态
  private status: 'paused' | 'running' = 'paused';
  // 最大并发数
  private concurrency: number = 4;

  constructor(concurrency: number = 4) {
    super();
    this.concurrency = concurrency;
  }

  // 添加任务
  add(...tasks: Task[]) {
    for (const t of tasks) {
      this.tasks.add(t);
    }
  }

  // 添加任务并启动执行
  addAndStart(...tasks: Task[]) {
    this.add(...tasks);
    this.start();
  }

  // 启动任务
  start() {
    if (this.status === 'running') {
      return; // 任务正在进行中，结束
    }
    if (this.tasks.size === 0) {
      // 当前已无任务，触发drain事件
      this.emit('drain');
      return;
    }
    // 设置任务状态为running
    this.status = 'running';
    this.emit('start'); // 触发start事件
    this.runNext(); // 开始执行下一个任务
  }

  // 取出第一个任务
  private takeHeadTask() {
    const task = this.tasks.values().next().value;
    if (task) {
      this.tasks.delete(task);
    }
    return task;
  }

  // 执行下一个任务
  private runNext() {
    if (this.status !== 'running') {
      return; // 如果整体的任务状态不是running，结束
    }
    if (this.currentCount >= this.concurrency) {
      // 并发数已满，结束
      return;
    }
    // 取出第一个任务
    const task = this.takeHeadTask();
    if (!task) {
      // 没有任务了
      this.status = 'paused'; // 暂停执行
      this.emit('drain'); // 触发drain事件
      return;
    }
    this.currentCount++; // 当前任务数+1
    // 执行任务
    Promise.resolve(task.run()).finally(() => {
      // 任务执行完成后，当前任务数-1，继续执行下一个任务
      this.currentCount--;
      this.runNext();
    });
  }

  // 暂停任务
  pause() {
    this.status = 'paused';
    this.emit('pause');
  }
}
```

### 前端要处理的问题

1. 如何分片
2. 控制请求

#### 如何对文件分片？

首先要实现分片对象的处理

```ts
// chunk.ts

export interface Chunk {
  blob: Blob; // 分片的二进制数据
  start: number; // 分片的起始位置
  end: number; // 分片的结束位置
  hash: string; // 分片的hash值
  index: number; // 分片在文件中的索引
}

// 创建一个不带hash的chunk
export function createChunk(
  file: File,
  index: number,
  chunkSize: number
): Chunk {
  const start = index * chunkSize;
  const end = Math.min((index + 1) * chunkSize, file.size);
  const blob = file.slice(start, end);
  return {
    blob,
    start,
    end,
    hash: '',
    index,
  };
}

// 计算chunk的hash值
export function calcChunkHash(chunk: Chunk): Promise<string> {
  return new Promise((resolve) => {
    const spark = new SparkMD5.ArrayBuffer();
    const fileReader = new FileReader();
    fileReader.onload = (e) => {
      spark.append(e.target?.result as ArrayBuffer);
      resolve(spark.end());
    };
    fileReader.readAsArrayBuffer(chunk.blob);
  });
}
```